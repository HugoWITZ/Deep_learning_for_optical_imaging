{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Termote/Deep_learning_for_optical_imaging/blob/main/Final_project_DefeatChatGPT_opti_WITZ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvr5d79ZS7QA"
      },
      "source": [
        "Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IhK9IDrfS-kW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D,TimeDistributed, Concatenate, Conv3D, MaxPooling3D, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import gdown\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRGb7uk0WSjs",
        "outputId": "00e0666c-c21b-4b96-fdff-ae53df592fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1zSda57x2TtlC0h-7VTOL18bpnQEn715y \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the data training folder\n",
        "url = \"https://drive.google.com/drive/folders/1ud-8x1rVE2pG2Tnh40okr33t47QfKNAc\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "0FuJyL8wnj5L",
        "outputId": "3256d274-68c3-4df5-ce84-3e07ca26289e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8c8a52abeb23>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Project Files/Training Data/common_time.npy'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# File paths and corresponding column names\n",
        "file_columns = [('/content/Project Files/Training Data/common_time.npy', 'common_time'),\n",
        "                ('/content/Project Files/Training Data/ground_truth.npy', 'ground_truth'),\n",
        "                ('/content/Project Files/Training Data/labels.npy', 'labels'),\n",
        "                ('/content/Project Files/Training Data/meteo_data.csv', 'meteo_data')]\n",
        "# Create an empty DataFrame\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Load and add data to the DataFrame\n",
        "for file_path, column_name in file_columns:\n",
        "    if file_path.endswith('.npy'):\n",
        "        data = np.load(file_path, allow_pickle=True)\n",
        "        df = pd.DataFrame(data, columns=[column_name])\n",
        "        combined_df = pd.concat([combined_df, df], axis=1)\n",
        "    elif file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "        df.columns = [f\"{column_name}_{col}\" for col in df.columns]\n",
        "        combined_df = pd.concat([combined_df, df], axis=1)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "\n",
        "#remove one of the date columns\n",
        "combined_df = combined_df.drop('meteo_data_time', axis=1)\n",
        "print(combined_df.shape)\n",
        "combined_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwnA2rShYYkJ"
      },
      "outputs": [],
      "source": [
        "# Load images\n",
        "x1 = np.load('/content/Project Files/Training Data/X1.npy')\n",
        "x2 = np.load('/content/Project Files/Training Data/X2.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJwrBALc-EyL"
      },
      "outputs": [],
      "source": [
        "# Create a subplot with two rows and five columns\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 5))\n",
        "\n",
        "# Display images in the subplot\n",
        "for i in range(5):\n",
        "    axes[0, i].imshow(x1[i])\n",
        "    axes[0, i].axis('off')\n",
        "    axes[1, i].imshow(x2[i])\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"x1 shape: \", x1.shape)\n",
        "print(\"x2 shape: \", x2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7qRFRu-ADXZ"
      },
      "outputs": [],
      "source": [
        "# Resizing the images to new dimensions\n",
        "\n",
        "# take only every two images and combined_df\n",
        "x1 = x1[::4]\n",
        "x2 = x2[::4]\n",
        "combined_df = combined_df.iloc[::4]\n",
        "\n",
        "new_dim = 100\n",
        "image_crop_percent = 0 # (top, bottom, left, right)\n",
        "side_dim_scaled = (new_dim, int(new_dim * (1 - image_crop_percent)))\n",
        "\n",
        "x1_resized = np.array([cv2.resize(img, (new_dim, new_dim)) for img in x1])\n",
        "del x1\n",
        "# crop the images\n",
        "x1_resized = x1_resized[:, 0:int(x1_resized.shape[1]*(1-image_crop_percent))]\n",
        "\n",
        "x2_resized = np.array([cv2.resize(img, (new_dim, new_dim)) for img in x2])\n",
        "del x2\n",
        "# crop the images\n",
        "x2_resized = x2_resized[:, 0:int(x2_resized.shape[1]*(1-image_crop_percent))]\n",
        "\n",
        "# normalize the images\n",
        "\n",
        "x1_resized = x1_resized / 255.0\n",
        "x2_resized = x2_resized / 255.0\n",
        "\n",
        "new_dim = (x1_resized.shape[1], x1_resized.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj7pcABLAVGq"
      },
      "outputs": [],
      "source": [
        "print(\"x1_resized shape: \", x1_resized.shape)\n",
        "print(\"x2_resized shape: \", x2_resized.shape)\n",
        "\n",
        "# Create a subplot with two rows and five columns\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "\n",
        "# Display images in the subplot\n",
        "for i in range(5):\n",
        "    axes[0, i].imshow(x1_resized[i])\n",
        "    axes[0, i].axis('off')\n",
        "    axes[1, i].imshow(x2_resized[i])\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpddo0UYzrfs"
      },
      "outputs": [],
      "source": [
        "#Split data\n",
        "\n",
        "def date_to_cyclic(day, month):\n",
        "    c_day = np.cos(day * (2 * np.pi / 31))\n",
        "    s_day = np.sin(day * (2 * np.pi / 31))\n",
        "    c_month = np.cos(month * (2 * np.pi / 12))\n",
        "    s_month = np.sin(month * (2 * np.pi / 12))\n",
        "    return c_day, s_day, c_month, s_month\n",
        "\n",
        "def split_train_test(data, train_test_ratio):\n",
        "    train_size = int(len(data) * train_test_ratio)\n",
        "    return data[:train_size], data[train_size:]\n",
        "\n",
        "def create_train_test(train_test_ratio):\n",
        "\n",
        "    date = combined_df['common_time']\n",
        "    # convert to datetime\n",
        "    date = pd.to_datetime(date)\n",
        "    # extract the month\n",
        "    month = date.dt.month\n",
        "    # extract the day\n",
        "    day = date.dt.day\n",
        "\n",
        "    # convert to cyclic\n",
        "    c_day, s_day, c_month, s_month = date_to_cyclic(day, month)\n",
        "    \n",
        "    columns = ['meteo_data_Wind_speed', 'meteo_data_Wind_dir', 'meteo_data_Air_temp','ground_truth']\n",
        "    meteo_data = combined_df[columns]\n",
        "    meteo_data = np.array(meteo_data)\n",
        "\n",
        "    # Add cyclic date to meteo_data\n",
        "    meteo_data = np.concatenate((meteo_data, c_day[:, None], s_day[:, None], c_month[:, None], s_month[:, None]), axis=1)\n",
        "\n",
        "    labels = combined_df['labels']\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    train_x1, test_x1 = split_train_test(x1_resized, train_test_ratio)\n",
        "    train_x2, test_x2 = split_train_test(x2_resized, train_test_ratio)\n",
        "\n",
        "    train_data, test_data = split_train_test(meteo_data, train_test_ratio)\n",
        "\n",
        "    y_train, y_test = split_train_test(labels, train_test_ratio)\n",
        "\n",
        "    X_images_train = np.stack((train_x1, train_x2), 1)\n",
        "    X_images_test = np.stack((test_x1, test_x2), 1)\n",
        "\n",
        "    # Shuffle the data for training\n",
        "    idx = np.random.permutation(len(X_images_train))\n",
        "\n",
        "    X_images_train, X_data_train, y_train = X_images_train[idx], train_data[idx], y_train[idx]\n",
        "\n",
        "    return X_images_train, X_data_train, X_images_test, test_data, y_train, y_test\n",
        "\n",
        "\n",
        "train_test_ratio = 0.9\n",
        "\n",
        "X_images_train, X_data_train, X_images_test, X_data_test, y_train, y_test = create_train_test(train_test_ratio)\n",
        "\n",
        "print(\"X_images_train shape:\", X_images_train.shape)\n",
        "print(\"X_data_train shape:\", X_data_train.shape)\n",
        "print(\"X_images_test shape:\", X_images_test.shape)\n",
        "print(\"X_data_test shape:\", X_data_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "del x1_resized\n",
        "del x2_resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dyipqd8FSxm"
      },
      "outputs": [],
      "source": [
        "# print the first 2 images with labels and corresponding wind speed\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
        "\n",
        "# Display images in the subplot\n",
        "for i in range(4):\n",
        "\n",
        "    axes[0, i].imshow(X_images_train[i][0])\n",
        "    axes[0, i].axis('off')\n",
        "    # plot title as label and wind speed\n",
        "    axes[0, i].set_title(\"Label: {} \\nWind speed: {}\\nWind direction: {}\\nAir temp: {}\\nGround Truth: {}\".format(y_train[i], X_data_train[i,0],X_data_train[i,1],X_data_train[i,2],X_data_train[i,3]))\n",
        "    axes[1, i].imshow(X_images_train[i][1])\n",
        "    axes[1, i].axis('off')\n",
        "    axes[1, i].set_title(\"Label: {} \\nWind speed: {}\\nWind direction: {}\\nAir temp: {}\\nGround Truth: {}\".format(y_train[i], X_data_train[i,0],X_data_train[i,1],X_data_train[i,2],X_data_train[i,3]))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5DkOt6sSqpK"
      },
      "source": [
        "ChatGPT model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJr7Nwp1R9o5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, LSTM, Dense, Flatten, concatenate\n",
        "import numpy as np\n",
        "# import exponentialdecay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU68TYKqIUSo"
      },
      "outputs": [],
      "source": [
        "channels = 3\n",
        "\n",
        "input_image = tf.keras.layers.Input(shape=(2,new_dim[0], new_dim[1], channels), name='image_input')\n",
        "\n",
        "input_data = tf.keras.layers.Input(shape=(8,), name='weather_data_input')\n",
        "\n",
        "# Hidden layers for data input\n",
        "data_input = tf.keras.layers.Dense(8, activation='relu')(input_data)\n",
        "data_input = tf.keras.layers.Dense(32, activation='relu')(input_data)\n",
        "data_input = tf.keras.layers.Dense(8, activation='relu')(data_input)\n",
        "\n",
        "# CNN Block 1\n",
        "first_input = tf.keras.layers.TimeDistributed(Conv2D(8, kernel_size=3, activation='relu'))(input_image)\n",
        "first_input = tf.keras.layers.TimeDistributed(MaxPooling2D(pool_size = 2))(first_input)\n",
        "first_input = tf.keras.layers.TimeDistributed(Conv2D(8, kernel_size=3, activation='relu'))(first_input)\n",
        "first_input = tf.keras.layers.TimeDistributed(MaxPooling2D(pool_size = 2))(first_input)\n",
        "\n",
        "# CNN Block 2\n",
        "first_input = tf.keras.layers.TimeDistributed(Conv2D(16, kernel_size=3, activation='relu'))(first_input)\n",
        "first_input = tf.keras.layers.TimeDistributed(MaxPooling2D(pool_size = 2))(first_input)\n",
        "first_input = tf.keras.layers.TimeDistributed(Conv2D(16, kernel_size=3, activation='relu'))(first_input)\n",
        "first_input = tf.keras.layers.TimeDistributed(MaxPooling2D(pool_size = 2))(first_input)\n",
        "\n",
        "# Flattening and Reshaping for LSTM\n",
        "flt = tf.keras.layers.TimeDistributed(Flatten())(first_input)\n",
        "flt = tf.keras.layers.Reshape((1, -1))(flt)\n",
        "\n",
        "# Reshape data input for concatenation\n",
        "data_input = tf.keras.layers.Reshape((1, -1))(data_input)\n",
        "\n",
        "# Concatenating CNN and data input\n",
        "concat = tf.keras.layers.concatenate([flt, data_input])\n",
        "\n",
        "# LSTM\n",
        "lstm1 = tf.keras.layers.LSTM(64, activation='relu', return_sequences=False)(concat)\n",
        "\n",
        "# Output\n",
        "dense1 = tf.keras.layers.Dense(1, activation='relu')(lstm1)\n",
        "\n",
        "# Setting input and ouput of model\n",
        "model = tf.keras.models.Model(inputs=[input_image, input_data], outputs=dense1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvZ5kJkEwi0G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "# compile the model and use mse as loss function, use as metric accuracy or mse itself \n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0001, decay_steps=250, decay_rate=0.9\n",
        ")\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mean_squared_error')\n",
        "\n",
        "print('Graphical schematic of the compiled network')\n",
        "print()\n",
        "model.summary()\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "Image(filename='model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKEwV2SBxHyV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rmse_list = []\n",
        "\n",
        "def reset_model(model):\n",
        "\n",
        "    model_copy = tf.keras.models.clone_model(model)\n",
        "    model_copy.set_weights(model.get_weights())\n",
        "\n",
        "    return model_copy\n",
        "\n",
        "for i in range(5):\n",
        "    print('Training iteration: ', i+1)\n",
        "    print()\n",
        "\n",
        "    model_copy = reset_model(model)\n",
        "    model_copy.compile(optimizer='adam', loss='mean_squared_error') # default lr = 0.001\n",
        "\n",
        "    history = model_copy.fit([X_images_train, X_data_train], y_train, shuffle=True, epochs=50, batch_size=128, verbose = 2, validation_split=0.1)\n",
        "    print()\n",
        "    print('Training rmse: ', np.sqrt(history.history['loss'][-1]))\n",
        "    print('Validation rmse: ', np.sqrt(history.history['val_loss'][-1]))\n",
        "\n",
        "    y_pred = model_copy.predict([X_images_test, X_data_test])\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    print('Test accuracy: ', rmse)\n",
        "    print()\n",
        "    rmse_list.append(rmse)\n",
        "    if i != 4 :\n",
        "      del model_copy\n",
        "      del history\n",
        "\n",
        "#Averaging the test accuracy after 5 times training\n",
        "print('Average test rmse: ', np.mean(rmse_list))\n",
        "print('Standard deviation of test rmse: ', np.std(rmse_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgsSHYFNLaKV"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrz2vbL5Li5A"
      },
      "outputs": [],
      "source": [
        "# Plot the RMSE loss\n",
        "\n",
        "plt.plot(np.sqrt(history.history['loss']), label='RMSE Training loss')\n",
        "plt.plot(np.sqrt(history.history['val_loss']), label='RMSE Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmD3p2LYJFpn"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = model_copy.predict([X_images_test, X_data_test])\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwTFkzp2KcSD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}